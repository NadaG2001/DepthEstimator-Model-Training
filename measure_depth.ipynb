{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0XQaohTYZDXf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUSYbXoqppEJ",
        "outputId": "bbe086bf-833c-4c9d-80ee-4704db226d39"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KwU4IctwkzB7"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "class SimpleDepthEstimator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleDepthEstimator, self).__init__()\n",
        "        # Use a pre-trained model as a feature extractor\n",
        "        self.features = models.resnet18(pretrained=True)\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "        # Re-activate gradient updates for last layers\n",
        "        for param in self.features.layer4.parameters():\n",
        "            param.requires_grad = True\n",
        "        self.features.fc = nn.Identity()  # Remove the classification head\n",
        "\n",
        "        # Add custom layers for depth estimation\n",
        "        self.depth_layers = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)  # Output a single depth value for simplicity\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)  # Extract features\n",
        "        depth = self.depth_layers(x)  # Estimate depth\n",
        "        return depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lEllE-_jk0C6"
      },
      "outputs": [],
      "source": [
        "# Define transformations for training and validation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qhzyUd7MlDxM"
      },
      "outputs": [],
      "source": [
        "# Load all images from the dataset folder\n",
        "data_folder = 'data22555'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vkp3oefTm92y"
      },
      "outputs": [],
      "source": [
        "image_filenames = os.listdir(data_folder)\n",
        "images = []\n",
        "for filename in image_filenames:\n",
        "    img_path = os.path.join(data_folder, filename)\n",
        "    image = Image.open(img_path)\n",
        "    images.append(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P1xMi3YsqUTf"
      },
      "outputs": [],
      "source": [
        "# Split the data into training, validation, and testing sets\n",
        "train_images, test_val_images = train_test_split(images, test_size=0.4, random_state=42)\n",
        "val_images, test_images = train_test_split(test_val_images, test_size=0.5, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KpVslS3mqUW-"
      },
      "outputs": [],
      "source": [
        "# Dataset class for tire images\n",
        "class TireDataset(Dataset):\n",
        "    \"\"\"Basic Tire Image Dataset\"\"\"\n",
        "\n",
        "    def __init__(self, images, transform=None):\n",
        "        self.images = images\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            transform = transforms.ToTensor()\n",
        "            image = transform(image)\n",
        "\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6Don-HZ9q7Mh"
      },
      "outputs": [],
      "source": [
        "# Dataset and dataloader setup\n",
        "train_dataset = TireDataset(train_images, transform=train_transform)\n",
        "val_dataset = TireDataset(val_images, transform=val_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByGM--CwrO4j",
        "outputId": "61a756b4-b9ae-4a0e-d654-bc7bbfecf3b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 50\n",
            "Testing set size: 17\n",
            "Validation set size: 17\n"
          ]
        }
      ],
      "source": [
        "print(f\"Training set size: {len(train_images)}\")\n",
        "print(f\"Testing set size: {len(test_images)}\")\n",
        "print(f\"Validation set size: {len(val_images)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9kW9aNWrO8D",
        "outputId": "a64d37f6-47a3-4a13-ddfb-f525a0b951ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\free bytes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\free bytes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the model\n",
        "model = SimpleDepthEstimator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "X2_ci05Or8C-"
      },
      "outputs": [],
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam([{'params': model.features.layer4.parameters(), 'lr': 1e-4},\n",
        "                        {'params': model.depth_layers.parameters()}], lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mki10Yror8Gd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/25], Train Loss: 0.1298, Val Loss: 0.0113\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\free bytes\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/25], Train Loss: 0.2090, Val Loss: 0.0135\n",
            "Epoch [3/25], Train Loss: 0.1356, Val Loss: 0.0196\n",
            "Epoch [4/25], Train Loss: 0.0981, Val Loss: 0.0311\n",
            "Epoch [5/25], Train Loss: 0.0927, Val Loss: 0.0468\n",
            "Epoch [6/25], Train Loss: 0.0838, Val Loss: 0.0857\n",
            "Epoch [7/25], Train Loss: 0.0913, Val Loss: 0.0656\n",
            "Epoch [8/25], Train Loss: 0.0865, Val Loss: 0.0333\n",
            "Epoch [9/25], Train Loss: 0.0669, Val Loss: 0.0369\n",
            "Epoch [10/25], Train Loss: 0.1088, Val Loss: 0.0149\n",
            "Epoch [11/25], Train Loss: 0.0564, Val Loss: 0.0136\n",
            "Epoch [12/25], Train Loss: 0.0601, Val Loss: 0.0302\n",
            "Epoch [13/25], Train Loss: 0.0507, Val Loss: 0.0187\n",
            "Epoch [14/25], Train Loss: 0.0349, Val Loss: 0.0152\n",
            "Epoch [15/25], Train Loss: 0.0423, Val Loss: 0.0212\n",
            "Epoch [16/25], Train Loss: 0.0294, Val Loss: 0.0198\n",
            "Epoch [17/25], Train Loss: 0.0317, Val Loss: 0.0140\n",
            "Epoch [18/25], Train Loss: 0.0256, Val Loss: 0.0171\n",
            "Epoch [19/25], Train Loss: 0.0249, Val Loss: 0.0136\n",
            "Epoch [20/25], Train Loss: 0.0268, Val Loss: 0.0168\n",
            "Epoch [21/25], Train Loss: 0.0283, Val Loss: 0.0143\n",
            "Epoch [22/25], Train Loss: 0.0276, Val Loss: 0.0094\n",
            "Epoch [23/25], Train Loss: 0.0302, Val Loss: 0.0087\n",
            "Epoch [24/25], Train Loss: 0.0242, Val Loss: 0.0099\n",
            "Epoch [25/25], Train Loss: 0.0301, Val Loss: 0.0131\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 25\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, torch.zeros_like(outputs))  # Dummy target for depth estimation\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, torch.zeros_like(outputs))  # Dummy target for depth estimation\n",
        "\n",
        "            running_val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
        "\n",
        "    scheduler.step(epoch_val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gbm2eq59x1H0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FM2N-lzDx1Ld"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
